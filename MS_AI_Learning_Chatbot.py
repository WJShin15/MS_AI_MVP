import asyncio
import os
from typing import List, Dict, Any
import openai
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.models import VectorQuery
from langchain_community.tools.tavily_search import TavilySearchResults
import streamlit as st
from dataclasses import dataclass
import logging
from dotenv import load_dotenv
import re

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentSummary:
    source: str
    content: str
    summary: str

class RAGChatbotWithWebSearch:
    def __init__(self):
        """RAG ì±—ë´‡ ì´ˆê¸°í™”"""
        # Azure ì„¤ì •
        self.search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
        self.search_key = os.getenv("AZURE_AI_SEARCH_API_KEY")
        self.search_index = os.getenv("AZURE_AI_SEARCH_INDEX_NAME", "rag-msai-learn03")
        
        # OpenAI ì„¤ì • (Azure OpenAI Service)
        self.openai_client = openai.AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2024-12-01-preview"
        )
        
        # ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.search_index,
            credential=AzureKeyCredential(self.search_key)
        )
        
        # Tavily ì›¹ê²€ìƒ‰ ì´ˆê¸°í™”
        self.tavily = TavilySearchResults(
            max_results=5,
            api_key=os.getenv("TAVILY_API_KEY")
        )
    
    def generate_query_embedding(self, query: str, deployment_name: str) -> List[float]:
        """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            response = self.openai_client.embeddings.create(
                model=deployment_name,
                input=query
            )
            embedding = response.data[0].embedding
            return embedding
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def vector_search(self, query: str, rag_params: dict, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search for open-source frameworks, Azure OpenAI, RAG (using Azure AI Search) and LangChain/LangGraph information within PDF files."""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            embedding_model = os.getenv("OPENAI_EMBEDDING_DEPLOYMENT_NAME")
            embedding = self.generate_query_embedding(query, embedding_model)
            if not embedding:
                logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []

            # ë²¡í„° ì¿¼ë¦¬ ìƒì„± ë° ê²€ìƒ‰
            vector_query = {
                "vector": embedding,
                "k": top_k,
                "fields": "text_vector",  # ì‹¤ì œ ì¸ë±ìŠ¤ì˜ ë²¡í„° í•„ë“œëª…ì— ë§ê²Œ ìˆ˜ì •
                "kind": "vector"
            }
            results = self.search_client.search(
                search_text="",  # ë²¡í„° ì „ìš© ì¿¼ë¦¬ì´ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´
                vector_queries=[vector_query]
            )
            documents = []
            for i, doc in enumerate(results):
                documents.append({
                    'id': doc.get('chunk_id', f'doc_{i+1}'),
                    'content': doc.get('chunk', ''),  # ì‹¤ì œ ë³¸ë¬¸ í•„ë“œëª…ì— ë§ê²Œ ìˆ˜ì •
                    'title': doc.get('title', ''),
                    'source': 'AzureAISearch'
                })
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ ë°˜í™˜")
            #logger.info(f"ë²¡í„° ê²€ìƒ‰ ì›ë³¸ doc: {documents}")
            return documents
        except Exception as e:
            logger.error(f"Azure Cognitive Search ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
 
    def curriculum_vector_search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """Search information dedicated to Curriculum, Schedule, and Progress from PDF files."""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            embedding_model = os.getenv("OPENAI_EMBEDDING_DEPLOYMENT_NAME")
            embedding = self.generate_query_embedding(query, embedding_model)
            if not embedding:
                logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []

            # curriculum ì¸ë±ìŠ¤ìš© í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            curriculum_index = os.getenv("AZURE_AI_CURRICULUM_INDEX_NAME", "rag-curriculum")
            curriculum_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=curriculum_index,
                credential=AzureKeyCredential(self.search_key)
            )
            vector_query = {
                "vector": embedding,
                "k": top_k,
                "fields": "text_vector",  # ì‹¤ì œ ì¸ë±ìŠ¤ì˜ ë²¡í„° í•„ë“œëª…ì— ë§ê²Œ ìˆ˜ì •
                "kind": "vector"
            }

            # ì¿¼ë¦¬ì—ì„œ ì¼ì°¨ ì •ë³´ ì¶”ì¶œ
            day_pattern = r'(\d+)ì¼ì°¨'
            day_match = re.search(day_pattern, query)
            target_day = day_match.group(0) if day_match else None
            
            logger.info(f"ì¶”ì¶œëœ ëŒ€ìƒ ì¼ì°¨: {target_day}")

            results = curriculum_client.search(
                search_text=query,  # í‚¤ì›Œë“œ ê²€ìƒ‰ë„ í•¨ê»˜ í™œìš©
                vector_queries=[vector_query],
                top=top_k,
                select=["chunk_id", "chunk", "title"]  # í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ
            )
            
            documents = []
            for i, doc in enumerate(results):
                # ê²€ìƒ‰ ìŠ¤ì½”ì–´ í™•ì¸ (ë””ë²„ê¹…ìš©)
                score = getattr(doc, '@search.score', None)
                
                # í•„ë“œ ë§¤í•‘ í™•ì¸ ë° ì•ˆì „í•œ ì ‘ê·¼
                chunk_id = doc.get('chunk_id') or doc.get('id') or f'curriculum_doc_{i+1}'
                content = doc.get('chunk') or doc.get('content') or doc.get('text') or ''
                title = doc.get('title') or doc.get('document_title') or 'ì œëª© ì—†ìŒ'
                #source = doc.get('source') or 'CurriculumIndex'
                
                if content:
                    # ì¼ì°¨ë³„ í•„í„°ë§ ë¡œì§ - ì–¸ê¸‰ íšŸìˆ˜ ê¸°ë°˜
                    if target_day:
                        # ëŒ€ìƒ ì¼ì°¨ ì–¸ê¸‰ íšŸìˆ˜ ê³„ì‚°
                        target_day_count = content.count(target_day) + title.count(target_day)
                        
                        # ë‹¤ë¥¸ ì¼ì°¨ë“¤ì˜ ì–¸ê¸‰ íšŸìˆ˜ ê³„ì‚°
                        other_days_pattern = r'(\d+)ì¼ì°¨'
                        other_days = re.findall(other_days_pattern, content + title)
                        target_day_num = target_day.replace('ì¼ì°¨', '')
                        
                        # ë‹¤ë¥¸ ì¼ì°¨ë³„ ì–¸ê¸‰ íšŸìˆ˜ ê³„ì‚°
                        other_days_count = {}
                        for day in other_days:
                            if day != target_day_num:
                                other_days_count[day] = other_days_count.get(day, 0) + 1
                        
                        # ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ë‹¤ë¥¸ ì¼ì°¨ì˜ íšŸìˆ˜
                        max_other_day_count = max(other_days_count.values()) if other_days_count else 0
                        
                        # ëŒ€ìƒ ì¼ì°¨ ì–¸ê¸‰ ë¹„ìœ¨ ê³„ì‚° (ì „ì²´ ì¼ì°¨ ì–¸ê¸‰ ì¤‘ ëŒ€ìƒ ì¼ì°¨ ë¹„ìœ¨)
                        total_day_mentions = target_day_count + sum(other_days_count.values())
                        target_day_ratio = target_day_count / total_day_mentions if total_day_mentions > 0 else 0
                        
                        # ë¬¸ì„œì— ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€
                        relevance_score = target_day_count * 10 + target_day_ratio * 100
                        
                        documents.append({
                            'id': chunk_id,
                            'content': content,
                            'title': title,
                            'source': 'CurriculumIndex',
                            'target_day_count': target_day_count,
                            'max_other_day_count': max_other_day_count,
                            'target_day_ratio': target_day_ratio,
                            'relevance_score': relevance_score,
                            'relevance_reason': f"Target day mentioned {target_day_count} times, ratio: {target_day_ratio:.2f}"
                        })
                    else:
                        # target_dayê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶”ê°€
                        documents.append({
                            'id': chunk_id,
                            'content': content,
                            'title': title,
                            'source': 'CurriculumIndex',
                            'target_day_count': 0,
                            'max_other_day_count': 0,
                            'target_day_ratio': 0,
                            'relevance_score': score if score else 0,
                            'relevance_reason': 'No specific day filtering'
                        })

                # ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬ (ëŒ€ìƒ ì¼ì°¨ ì–¸ê¸‰ íšŸìˆ˜ì™€ ë¹„ìœ¨ ê¸°ì¤€)
                if target_day:
                    documents.sort(key=lambda x: (
                        x['target_day_count'],          # 1ìˆœìœ„: ëŒ€ìƒ ì¼ì°¨ ì–¸ê¸‰ íšŸìˆ˜
                        x['target_day_ratio'],          # 2ìˆœìœ„: ëŒ€ìƒ ì¼ì°¨ ì–¸ê¸‰ ë¹„ìœ¨
                        -x['max_other_day_count'],      # 3ìˆœìœ„: ë‹¤ë¥¸ ì¼ì°¨ ì–¸ê¸‰ì´ ì ì€ ìˆœ (ìŒìˆ˜ë¡œ ì—­ìˆœ)
                    ), reverse=True)
                    

                # ìµœì¢… ê²°ê³¼ë¥¼ 2ë¡œ ì œí•œ
                documents = documents[:2]

            logger.info(f"ì»¤ë¦¬í˜ëŸ¼ ì¸ë±ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ ë°˜í™˜")
            
            # # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸
            # for i, doc in enumerate(documents[:2]):  # ìƒìœ„ 2ê°œë§Œ ë¡œê¹…
            #     logger.info(f"ì»¤ë¦¬í˜ëŸ¼ ë¬¸ì„œ {i+1} - ì œëª©: {doc['title'][:50]}...")
            #     logger.info(f"ì»¤ë¦¬í˜ëŸ¼ ë¬¸ì„œ {i+1} - ë‚´ìš© ê¸¸ì´: {len(doc['content'])}")
            #     logger.info(f"ì»¤ë¦¬í˜ëŸ¼ ë¬¸ì„œ {i+1} - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc['content'][:200]}...")
            
            return documents

        except Exception as e:
            logger.error(f"ì»¤ë¦¬í˜ëŸ¼ ì¸ë±ìŠ¤ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
        
    def web_search(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """Tavilyë¥¼ ì´ìš©í•œ ì›¹ ê²€ìƒ‰"""
        try:
            # TavilySearchResultsì˜ run ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
            results = self.tavily.run(query)
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            web_content = ""
            if results:
                for i, result in enumerate(results[:max_results]):
                    title = result.get('title', 'No title')
                    content = result.get('content', 'No content')
                    url = result.get('url', 'No URL')
                    
                    web_content += f"\n=== ì›¹ ê²°ê³¼ {i+1} ===\n"
                    web_content += f"ì œëª©: {title}\n"
                    web_content += f"URL: {url}\n"
                    web_content += f"ë‚´ìš©: {content}\n"
            
            logger.info(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return {
                'content': web_content,
                'source': 'Web Search Results',
                'query': query
            }
            
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'content': f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                'source': 'Web Search Error',
                'query': query
            }

    async def summarize_content(self, content: str, source: str, context: str = "") -> DocumentSummary:
        """ê°œë³„ ë¬¸ì„œ/ì›¹ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (ë¹„ë™ê¸°)"""
        try:
            # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìš”ì•½í•˜ì§€ ì•ŠìŒ
            if len(content) < 100:
                return DocumentSummary(
                    source=source,
                    content=content,
                    summary=content
                )
            
            prompt = f"""
                    ë‹¤ìŒ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
                    ì»¨í…ìŠ¤íŠ¸: {context}
                    ì¶œì²˜: {source}

                    ë‚´ìš©:
                    {content[:3000]}  # í† í° ì œí•œì„ ìœ„í•´ 3000ìë¡œ ì œí•œ

                    ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
                    1. í•µì‹¬ ì •ë³´ë§Œ í¬í•¨
                    2. 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
                    3. ì¶œì²˜ íŠ¹ì„± ë°˜ì˜ (ë¬¸ì„œ vs ì›¹ê²€ìƒ‰)
                    4. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± ì¤‘ì‹¬

                    ìš”ì•½:"""

            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.openai_client.chat.completions.create(
                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "dev-gpt-4.1"),
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¬¸ì„œ ìš”ì•½ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=500,
                    temperature=0.4
                )
            )
            
            summary = response.choices[0].message.content.strip()
            
            return DocumentSummary(
                source=source,
                content=content,
                summary=summary
            )
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜ ({source}): {e}")
            return DocumentSummary(
                source=source,
                content=content,
                summary=f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {content[:200]}..."
            )
    

    async def process_query(self, user_query: str) -> str:
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜: ì¿¼ë¦¬ -> ê²€ìƒ‰ -> ë¹„ë™ê¸° ìš”ì•½ -> ìµœì¢… ë‹µë³€"""
        logger.info(f"ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {user_query}")

        # ì»¤ë¦¬í˜ëŸ¼/ì¼ì • ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ ë° ì§ˆë¬¸ ì¬êµ¬ì„±
        keywords = ['ì»¤ë¦¬í˜ëŸ¼', 'ì¼ì •', 'ìŠ¤ì¼€ì¤„', 'ìˆ˜ì—…', 'í•™ìŠµ', 'êµìœ¡', 'ì§„ë„']
        is_curriculum_query = any(k in user_query for k in keywords)
        course = st.session_state.user_info['course']
        new_query = user_query
        if is_curriculum_query and course:
            if 'ì˜¤ëŠ˜' in user_query:
                new_query = user_query.replace('ì˜¤ëŠ˜', course + ' ì»¤ë¦¬í˜ëŸ¼')
            #new_query = f"{course} ì»¤ë¦¬í˜ëŸ¼ {user_query}"
            #logger.info(f"ì»¤ë¦¬í˜ëŸ¼/ì¼ì • ì§ˆë¬¸ ì¬êµ¬ì„±: {new_query}")

        if is_curriculum_query:
            # ì»¤ë¦¬í˜ëŸ¼/ì¼ì • ì§ˆë¬¸ì´ë©´ curriculum_vector_searchë§Œ ì‹¤í–‰
            logger.info("ì»¤ë¦¬í˜ëŸ¼/ì¼ì • ì§ˆë¬¸: curriculum_vector_searchë§Œ ì‹¤í–‰")
            documents = self.curriculum_vector_search(new_query, top_k=3)
            web_results = {'content': '', 'source': 'Web Search Skipped', 'query': new_query}
        else:
            # í‰ì†Œì—ëŠ” ë²¡í„°+ì›¹ ê²€ìƒ‰ ë³‘í–‰
            rag_params = {
                "data_sources": [
                    {
                        "type": "azure_search",
                        "parameters": {
                            "endpoint": os.getenv("AZURE_AI_SEARCH_ENDPOINT"),
                            "index_name": os.getenv("AZURE_AI_SEARCH_INDEX_NAME"),
                            "authentication": {
                                "type": "api_key",
                                "key": os.getenv("AZURE_AI_SEARCH_API_KEY")
                            },
                            "query_type": "vector",
                            "embedding_dependency": {
                                "type": "deployment_name",
                                "deployment_name": os.getenv("OPENAI_EMBEDDING_DEPLOYMENT_NAME")
                            }
                        }
                    }
                ]
            }
            logger.info("ì¼ë°˜ ì§ˆë¬¸: ë²¡í„°+ì›¹ ê²€ìƒ‰ ë³‘í–‰")
            vector_task = asyncio.get_event_loop().run_in_executor(
                None, lambda: self.vector_search(user_query, rag_params, 5)
            )
            web_task = asyncio.get_event_loop().run_in_executor(
                None, self.web_search, user_query, 5
            )
            documents, web_results = await asyncio.gather(vector_task, web_task)

        # 2ë‹¨ê³„: ëª¨ë“  ë‚´ìš©ì„ ë¹„ë™ê¸°ë¡œ ë™ì‹œì— ìš”ì•½
        logger.info("ë¹„ë™ê¸° ìš”ì•½ ì‹œì‘...")
        summarization_tasks = []

        # ë²¡í„° ê²€ìƒ‰ ë¬¸ì„œë“¤ ìš”ì•½ íƒœìŠ¤í¬
        for i, doc in enumerate(documents):
            task = self.summarize_content(
                content=doc['content'],
                source=f"ë¬¸ì„œ {i+1}: {doc['title']}",
                context=user_query
            )
            #logger.info(f"ë²¡í„° ê²€ìƒ‰ ì›ë³¸ ì œëª©: {doc['title']}")
            #logger.info(f"ë²¡í„° ê²€ìƒ‰ ì›ë³¸ ë¬¸ì„œ: {doc['content']}")
            summarization_tasks.append(task)

        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ íƒœìŠ¤í¬ (ì›¹ ê²€ìƒ‰ì´ ìŠ¤í‚µëœ ê²½ìš° ë¹ˆ ë‚´ìš©)
        web_summary_task = self.summarize_content(
            content=web_results['content'],
            source=web_results['source'],
            context=user_query
        )
        summarization_tasks.append(web_summary_task)

        # ëª¨ë“  ìš”ì•½ì„ ë™ì‹œì— ì‹¤í–‰
        summaries = await asyncio.gather(*summarization_tasks)

        # 3ë‹¨ê³„: ìš”ì•½ë“¤ì„ í•©ì³ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
        logger.info("ìµœì¢… ë‹µë³€ ìƒì„±...")
        return await self.generate_final_answer(user_query, summaries, is_curriculum_query)

    # ê¸°ì¡´ generate_final_answer í•¨ìˆ˜ë¥¼ ì´ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”

    async def generate_final_answer(self, user_query: str, summaries: List[DocumentSummary], is_curriculum_query: bool = False) -> str:
        """ìš”ì•½ëœ ë‚´ìš©ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (ì¶œì²˜ í‘œê¸° í¬í•¨, êµ¬ì¡°í™”ëœ í˜•ì‹)"""
        try:
            # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - ì¶œì²˜ ì •ë³´ í¬í•¨
            combined_summaries = ""
            source_mapping = []
            
            for i, summary in enumerate(summaries):
                if "Web Search" in summary.source:
                    source_label = "ì›¹ ê²€ìƒ‰"
                    source_mapping.append("ì›¹ ê²€ìƒ‰: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ê²°ê³¼")
                else:
                    source_label = f"ë¬¸ì„œ {i+1}"
                    # ì œëª© ì¶”ì¶œ
                    if ':' in summary.source:
                        title = summary.source.split(':', 1)[1].strip()
                    else:
                        title = summary.source.strip()
                    source_mapping.append(f"ë¬¸ì„œ {i+1}: {title}")
                
                combined_summaries += f"\n--- {source_label} ---\n{summary.summary}\n"

            if is_curriculum_query:
                    final_prompt = f"""
                    ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

                    ë‹¤ìŒì€ ê´€ë ¨ ì •ë³´ë“¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤:
                    {combined_summaries}

                    ìœ„ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ MS AI êµìœ¡ ì¤‘ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ **êµ¬ì¡°í™”ëœ í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì†Œì œëª©, ë¦¬ìŠ¤íŠ¸, ë²ˆí˜¸, í‘œ ë“±ì„ í™œìš©í•´ ê°€ë…ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.

                    ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:
                    1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•´ì•¼ í•¨.
                    2. ì»¤ë¦¬í˜ëŸ¼ ë° ì¼ì • ê´€ë ¨ ì§ˆë¬¸ì€ ë¬¸ì„œ ì •ë³´ë§Œ ê¸°ì¤€ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê³  ì¶œì²˜í‘œê¸°ëŠ” ìƒëµí•©ë‹ˆë‹¤. 
                    6. ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ ì œê³µ.
                    7. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±.
                    9. **êµ¬ì¡°í™”ëœ í˜•ì‹**:
                    - **ê°œìš”**: ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ë‹µë³€ ìš”ì•½ (1-2ë¬¸ì¥).
                    - **ì„¸ë¶€ ì •ë³´**: ê´€ë ¨ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸(ì˜ˆ: `-` ë˜ëŠ” `1.`)ë¡œ ì •ë¦¬.
                    - **ê¶Œì¥ ì‚¬í•­**: ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ì¸ ì¡°ì–¸ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ (ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ê¶Œì¥).

                    **êµ¬ì¡°í™”ëœ í˜•ì‹ ë‹µë³€ ì˜ˆì‹œ**:
                    ```markdown

                    ### ê°œìš”
                    [ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ë‹µë³€ ìš”ì•½]

                    ### ì„¸ë¶€ ì •ë³´
                    - [í•­ëª© 1]: [ì„¤ëª…]
                    - [í•­ëª© 2]: [ì„¤ëª…]
                    - [í•­ëª© 3]: [ì„¤ëª…]

                    ### ê¶Œì¥ ì‚¬í•­
                    1. [ê¶Œì¥ ì‚¬í•­ 1]
                    2. [ê¶Œì¥ ì‚¬í•­ 2]

                    ë‹µë³€:"""
            else:
                final_prompt = f"""
                ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

                ë‹¤ìŒì€ ê´€ë ¨ ì •ë³´ë“¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤:
                {combined_summaries}

                ìœ„ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ MS AI êµìœ¡ ì¤‘ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

                ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:
                1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ë‹µë³€ì€ ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤. 
                2. ì§ˆë¬¸ì€ ë¬¸ì„œ ì •ë³´ 60%ì™€ ì›¹ ê²€ìƒ‰ ì •ë³´ 40%ë¡œ ê· í˜• ìˆê²Œ í™œìš©.
                3. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ ìš°ì„  ì‚¬ìš©.
                4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” "ë¶ˆí™•ì‹¤" ë˜ëŠ” "ì¶”ê°€ í™•ì¸ í•„ìš”"ë¡œ ëª…ì‹œ.
                5. ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ ì œê³µ.
                6. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±.
                7. ì¶œì²˜ ê·œì¹™
                ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤, ì‹œê°„, ì ˆì°¨, ë‚´ìš©ì„ ì–¸ê¸‰í•  ë•ŒëŠ” í•´ë‹¹ ë¬¸ì¥ ëì— ì¶œì²˜ë¥¼ í‘œê¸°í•´ì£¼ì„¸ìš”.
                - ë¬¸ì„œ ì •ë³´ ì‚¬ìš© ì‹œ: (ë¬¸ì„œ 1), (ë¬¸ì„œ 2) ë“±
                - ì›¹ ê²€ìƒ‰ ì •ë³´ ì‚¬ìš© ì‹œ: (ì›¹ ê²€ìƒ‰)
                - ì—¬ëŸ¬ ì¶œì²˜ ì¡°í•© ì‹œ: (ë¬¸ì„œ 1, ì›¹ ê²€ìƒ‰), (ë¬¸ì„œ 1, ë¬¸ì„œ 2) ë“±
                - ì¼ë°˜ì ì¸ ì„¤ëª…ì´ë‚˜ ì—°ê²° ë¬¸ì¥ì—ëŠ” ì¶œì²˜ í‘œê¸° ìƒëµ 

                ë‹µë³€:"""

            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.openai_client.chat.completions.create(
                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "dev-gpt-4.1"),
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” MS AI êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": final_prompt.strip()}
                    ],
                    max_tokens=3000,
                    temperature=0.5
                )
            )
            
            final_answer = response.choices[0].message.content.strip()
            logger.info("ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # í•˜ë‹¨ì— ì¶œì²˜ ë§¤í•‘ ì •ë³´ ì¶”ê°€ (ì„ íƒì‚¬í•­)
            #final_answer += "\n\n" + "=" * 50
            if is_curriculum_query == False:
                final_answer += "\n\n**ì¶œì²˜ ì •ë³´**"
                for mapping in source_mapping:
                    final_answer += f"\nâ€¢ {mapping}"
            
            return final_answer
            
        except Exception as e:
            logger.error(f"ìµœì¢… ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def main():
    st.set_page_config(
        page_title="MS AI í•™ìŠµ ì±—ë´‡",
        page_icon="ğŸ¤–",
        layout="wide"
    )

    st.title("ğŸ¤– MS AI í•™ìŠµ ì±—ë´‡")
    st.markdown("RAG ë²¡í„° ê²€ìƒ‰ê³¼ ì‹¤ì‹œê°„ ì›¹ê²€ìƒ‰ì„ ê²°í•©í•œ ì§€ëŠ¥í˜• ì±—ë´‡ì…ë‹ˆë‹¤.")

    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    required_env_vars = [
        "AZURE_AI_SEARCH_ENDPOINT",
        "AZURE_AI_SEARCH_API_KEY", 
        "AZURE_OPENAI_ENDPOINT",
        "AZURE_OPENAI_KEY",
        "TAVILY_API_KEY",
        "OPENAI_EMBEDDING_DEPLOYMENT_NAME"
    ]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        st.error(f"ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
        st.stop()

    # ì‚¬ìš©ì ì •ë³´ ì…ë ¥ (ìµœì´ˆ 1íšŒ)
    if 'user_info' not in st.session_state:
        with st.form("user_info_form"):
            user_id = st.text_input("User ID")
            user_name = st.text_input("User Name")
            course = st.selectbox("ê³¼ì •ì„ ì„ íƒí•˜ì„¸ìš”", [f"{i+1}ì¼ì°¨" for i in range(9)])
            submitted = st.form_submit_button("ì •ë³´ ì €ì¥")
            if submitted and user_id and user_name and course:
                st.session_state.user_info = {
                    "user_id": user_id,
                    "user_name": user_name,
                    "course": course
                }
                st.session_state.chat_messages = []
                st.session_state.chat_exit = False
                st.session_state.chatbot = RAGChatbotWithWebSearch()
                st.success("ì‚¬ìš©ì ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            elif submitted:
                st.warning("ëª¨ë“  ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        st.stop()

    # ì±—ë´‡ ì´ˆê¸°í™”
    if 'chatbot' not in st.session_state:
        try:
            st.session_state.chatbot = RAGChatbotWithWebSearch()
        except Exception as e:
            st.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            st.stop()
    if 'chat_messages' not in st.session_state:
        st.session_state.chat_messages = []
    if 'chat_exit' not in st.session_state:
        st.session_state.chat_exit = False

    st.markdown(f"**ì‚¬ìš©ì:** {st.session_state.user_info['user_name']} ({st.session_state.user_info['user_id']}) / ê³¼ì •: {st.session_state.user_info['course']}")

    # ëŒ€í™” ì…ë ¥ ë° ì²˜ë¦¬ (í•˜ë‹¨ì— ìœ„ì¹˜)
    user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)", key="main_chat_input")
    if user_input:
        if user_input.lower() == "exit":
            st.session_state.chat_exit = True
            st.success("ì±—ë´‡ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            st.session_state.chat_messages.append({
                "user": st.session_state.user_info['user_name'],
                "query": user_input,
                "answer": None  # ë‹µë³€ì€ ì•„ë˜ì—ì„œ ìƒì„±
            })

    # ëŒ€í™” ë‚´ìš© ì¶œë ¥ (ìƒë‹¨ì— ìœ„ì¹˜)
    st.markdown("### ğŸ“ ëŒ€í™” ë‚´ìš©")
    if st.session_state.chat_messages:
        for i, msg in enumerate(st.session_state.chat_messages, 1):
            with st.chat_message("user"):
                st.write(msg["query"])
            # ë‹µë³€ì´ ì•„ì§ ì—†ìœ¼ë©´ spinnerë¥¼ ì¶œë ¥
            if msg["answer"] is None and i == len(st.session_state.chat_messages):
                with st.spinner("ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    try:
                        answer = asyncio.run(
                            st.session_state.chatbot.process_query(msg["query"])
                        )
                        st.session_state.chat_messages[-1]["answer"] = answer
                        with st.chat_message("assistant"):
                            st.write(answer)
                    except Exception as e:
                        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            elif msg["answer"]:
                with st.chat_message("assistant"):
                    st.write(msg["answer"])
    else:
        st.info("ì•„ì§ ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ëŒ€í™” ì¢…ë£Œ ì•ˆë‚´
    if st.session_state.chat_exit:
        st.info("ì±—ë´‡ ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ì‚¬ì´ë“œë°”ì— ì„¤ì • ì •ë³´
    with st.sidebar:
        st.markdown("### âš™ï¸ ì±—ë´‡ ì„¤ì • ì •ë³´")
        st.info("""
        **ê¸°ëŠ¥:**
        - ë°±í„° ì¸ë±ìŠ¤ ë¬¸ì„œ ê²€ìƒ‰ (5ê°œ)
        - ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ (5ê°œ ê²°ê³¼)
        - ë¹„ë™ê¸° ë³‘ë ¬ ìš”ì•½ ì²˜ë¦¬
        - í†µí•© ë‹µë³€ ìƒì„±
        - ì‚¬ìš©ì ì •ë³´ ë° ëŒ€í™” ë‚´ìš© ì €ì¥
        
        **ì²˜ë¦¬ ê³¼ì •:**
        1. ì‚¬ìš©ì ì •ë³´ ì…ë ¥
        2. ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„
        3. ë²¡í„° ê²€ìƒ‰ + ì›¹ ê²€ìƒ‰ (ë™ì‹œ ì‹¤í–‰)
        4. ìš”ì•½ ìƒì„± (ë¹„ë™ê¸°)
        5. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
        6. ìµœì¢… ë‹µë³€ ìƒì„±
        7. ëŒ€í™” ë‚´ìš© ì €ì¥ ë° ì¶œë ¥
        """)


if __name__ == "__main__":
    main()